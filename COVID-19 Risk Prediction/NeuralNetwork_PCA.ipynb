{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() \n",
    "filename = path+'/TrainSetPCA30.csv'\n",
    "trainSet = pd.read_csv(filename)\n",
    "filename = path+'/TestSetPCA30.csv'\n",
    "testSet = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = ['death30', 'death60', 'death90', 'ICU', 'vent', 'anyCatastrophic','Admit30Days', \n",
    "          'Admit60Days', 'Admit90Days', 'Admit7Days','Admit14Days']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating features (X) from labels (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = trainSet[Labels]\n",
    "y_train = y_train.astype(int)\n",
    "X_train = trainSet.drop(Labels,axis=1)\n",
    "y_test = testSet[Labels]\n",
    "y_test = y_test.astype(int)\n",
    "X_test = testSet.drop(Labels,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of features\n",
    "n_features = len(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focusing on one label to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting label: anyCatastrophic\n"
     ]
    }
   ],
   "source": [
    "labelName = Labels[5]\n",
    "labelTrain = y_train.loc[:,labelName]\n",
    "labelTest= y_test.loc[:,labelName]\n",
    "#display(labelTrain.value_counts())\n",
    "print(f'Predicting label: {labelName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, regularizers\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow import keras\n",
    "\n",
    "#Define the optimizer parameters, we tried several learning rates and these parameters gave the best results\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "elasticnet_regularizer= regularizers.l1_l2(l1=0.001, l2=0.001)\n",
    "\n",
    "# define the model\n",
    "#Our neural network has two hidden layers with variable number of neurons\n",
    "#The activation function was tested with sigmoid, tanh and relu. Relu gave the best results\n",
    "#We set a lasso regularizer to avoid overfitting as well as two dropout layers\n",
    "#Dropout rate was changed manually\n",
    "def create_model(n_neuronsL1,n_neuronsL2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neuronsL1, activation='relu', kernel_initializer='lecun_uniform',kernel_regularizer=elasticnet_regularizer,\n",
    "                    input_shape=(n_features,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_neuronsL2, activation='relu', kernel_initializer='lecun_uniform',kernel_regularizer=elasticnet_regularizer))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='binary_accuracy')\n",
    "    return model\n",
    "\n",
    "#The number of epochs and batchsize were set by manual search\n",
    "modelNN = KerasClassifier(build_fn=create_model, epochs=1000, batch_size=500, initial_epoch=0, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining parameter grid for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "neuronsL1 = range(10,20,2)\n",
    "neuronsL2 = range(2,10,2)\n",
    "\n",
    "param_grid = {'kerasclassifier__n_neuronsL1':neuronsL1,'kerasclassifier__n_neuronsL2':neuronsL2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This callback is not being used in the final version\n",
    "#from tensorflow import keras\n",
    "#es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define a pipeline to upsample the minority class on each training split of the crossvalidation\n",
    "#We upsample with a random sampler which copies random samples from the minority class\n",
    "#Then we pass the neural network model\n",
    "\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "pipeline_grid = make_pipeline(\n",
    "    RandomOverSampler(random_state=0),\n",
    "    modelNN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "#We perform a gridsearch over the hyperparameter grid defined above (number of neurons of layer 1 and 2)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef, f1_score\n",
    "\n",
    "scorer_mcc = make_scorer(matthews_corrcoef,greater_is_better=True)\n",
    "scoring = {'F1': 'f1', 'MCC': scorer_mcc}\n",
    "\n",
    "#Create cross-validation object for stratified splits\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#Perform gridsearch and score models based on f1 metric\n",
    "grid = GridSearchCV(pipeline_grid, param_grid=param_grid, n_jobs=-1,cv=kfold,scoring=scoring,refit = 'MCC',verbose=3)\n",
    "grid_result = grid.fit(X_train, labelTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "Best_params = grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_result.cv_results_)\n",
    "results = results.loc[:,['param_kerasclassifier__n_neuronsL1','param_kerasclassifier__n_neuronsL2','mean_test_F1',\n",
    "                         'rank_test_F1','mean_test_MCC','rank_test_MCC']]\n",
    "results['mean_test_score'] = results['mean_test_F1']\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "ax = plt.figure(figsize=(9,7)).gca()\n",
    "#Plot best estimator during training (based on f1 score)\n",
    "ax.scatter(Best_params['kerasclassifier__n_neuronsL1'], Best_params['kerasclassifier__n_neuronsL2'], s=200, marker= 'o',color='none',edgecolor='r')\n",
    "#Plot all models' scores\n",
    "\n",
    "\n",
    "\n",
    "heatmap_data = pd.pivot_table(results, values='mean_test_score', \n",
    "                     index=['param_kerasclassifier__n_neuronsL2'], \n",
    "                     columns='param_kerasclassifier__n_neuronsL1')\n",
    "#midpoint = (heatmap_data.values.max() - heatmap_data.values.min()) / 2\n",
    "\n",
    "ind_max = np.unravel_index(np.argmax(heatmap_data, axis=None), heatmap_data.shape)\n",
    "\n",
    "sns.set(font_scale=1.2) \n",
    "colormap = sns.diverging_palette(10, 240, n=20)\n",
    "ax = sns.heatmap(heatmap_data, annot=True,cmap =colormap,linewidths=.5,fmt='.3f',annot_kws={\"size\":15},robust=True)\n",
    "ax.add_patch(Rectangle((ind_max[1],ind_max[0]), 1, 1, fill=False, edgecolor='red', lw=3))\n",
    "\n",
    "#sc = ax.scatter(results.param_kerasclassifier__n_neuronsL1, results.param_kerasclassifier__n_neuronsL2, s=200, c=results.mean_test_score, cmap='Greens', marker= 'x')\n",
    "\n",
    "#ax.set_xticks(results.param_kerasclassifier__n_neuronsL1.tolist())\n",
    "#ax.set_yticks(results.param_kerasclassifier__n_neuronsL2.tolist())\n",
    "\n",
    "ax.set_xlabel(\"Number Neurons Layer 1\",fontsize=15)\n",
    "ax.set_ylabel(\"Number Neurons Layer 2\",fontsize=15)\n",
    "ax.set_title(\"F1 score\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing the model we use the complete training set to train again the models\n",
    "# The training set is upsampled on a similar way as during gridsearch\n",
    "from sklearn.utils import resample\n",
    "#The minority class is always the positive class\n",
    "majority_class = X_train[labelTrain==0].copy()\n",
    "majority_class_labeled = majority_class.join(labelTrain[labelTrain==0])\n",
    "minority_class = X_train[labelTrain==1].copy()\n",
    "minority_class_labeled = minority_class.join(labelTrain[labelTrain==1])\n",
    "\n",
    "# Upsample minority class\n",
    "minority_upsampled = resample(minority_class_labeled, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=11911,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "trainSet_upsampled = pd.concat([majority_class_labeled, minority_upsampled])\n",
    " \n",
    "labels_upsampled = trainSet_upsampled.loc[:,labelName]\n",
    "labels_upsampled.to_numpy()\n",
    "features_upsampled = trainSet_upsampled.drop(labelName,axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix,confusion_matrix, f1_score, classification_report, balanced_accuracy_score\n",
    "\n",
    "resultsTest = []\n",
    "global_results = pd.DataFrame()\n",
    "#Iterate through the combinations of parameters to test the model on the testing set\n",
    "#Requires to retrain the model for each parameter combination prior to testing\n",
    "\n",
    "for neurons1 in neuronsL1:\n",
    "    for neurons2 in neuronsL2:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(neurons1, activation='relu', kernel_initializer='lecun_uniform',kernel_regularizer='l1',\n",
    "                    input_shape=(n_features,)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(neurons2, activation='relu', kernel_initializer='lecun_uniform',kernel_regularizer='l1'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics='binary_accuracy')\n",
    "        model.fit(features_upsampled, labels_upsampled, epochs=1000, batch_size= 500, initial_epoch=0, verbose=0)\n",
    "        predictions = model.predict(X_test) > 0.5\n",
    "        testingScore = f1_score(labelTest,predictions,average='binary')\n",
    "        results_aux = dict(NeuronsLayer1=neurons1,NeuronsLayer2 = neurons2, TestingScore = testingScore)\n",
    "        resultsTest.append(results_aux)\n",
    "        print(results_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsTest= pd.DataFrame(resultsTest)\n",
    "\n",
    "#global_results stores the scores during training and testing and calculates the mean of both scores\n",
    "#The mean score is used to select the \"best overall model\"\n",
    "\n",
    "global_results = resultsTest.copy()\n",
    "global_results['TrainingScore'] = results.loc[:,'mean_test_score']\n",
    "col = global_results.loc[: , [\"TrainingScore\",\"TestingScore\"]]\n",
    "global_results['MeanScore'] = col.mean(axis=1)\n",
    "global_results\n",
    "\n",
    "#We find the best overall model (given by the maximum mean score bw training and testing)\n",
    "best_overall_model = global_results[global_results.MeanScore==global_results.MeanScore.max()]\n",
    "best_overall_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictability plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to find the min and max score of both training and testing\n",
    "#This is used to set the limits of the predictability plot axis\n",
    "def findlimits(training_score,testing_score):\n",
    "    min_training = training_score.min()\n",
    "    min_testing = testing_score.min()\n",
    "    min_global = np.min([min_training,min_testing])-0.005 #Adding 0.005 offset so that markers do not appear on the edge of plot\n",
    "    \n",
    "    max_training = training_score.max()\n",
    "    max_testing = testing_score.max()\n",
    "    max_global = np.max([max_training,max_testing])+0.005 #Adding 0.005 offset so that markers do not appear on the edge of plot\n",
    "    \n",
    "    return [min_global,max_global]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "#We find the training score for the best model during trainning\n",
    "best_training_result = results.mean_test_score.max()\n",
    "\n",
    "#We find the testing score for the best model during training\n",
    "testing_result = resultsTest.TestingScore[(resultsTest.NeuronsLayer1==Best_params['kerasclassifier__n_neuronsL1'])&\n",
    "                                   (resultsTest.NeuronsLayer2==Best_params['kerasclassifier__n_neuronsL2'])]\n",
    "\n",
    "ax = plt.figure(figsize=(7,7)).gca()\n",
    "\n",
    "#We mark the result for the best model during training\n",
    "ax.scatter(best_training_result,testing_result, s=200, marker= 'o',color='none',edgecolor='r')\n",
    "\n",
    "#We mark the result for the best overall model\n",
    "ax.scatter(best_overall_model.TrainingScore,best_overall_model.TestingScore, s=200, marker= 'o',color='none',edgecolor='green')\n",
    "\n",
    "#We plot all the scores for all models\n",
    "sc = ax.scatter(results.mean_test_score, resultsTest.TestingScore, s=50, marker= 'x',c='b')\n",
    "\n",
    "ax.set_xlim([0.25,0.45])\n",
    "ax.set_ylim([0.0,0.25])\n",
    "ax.set_xlabel(\"Training Score\",fontsize=15)\n",
    "ax.set_ylabel(\"Testing Score\",fontsize=15)\n",
    "ax.set_title(\"F1 score\",fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing best overall model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and test again the best overall model to get the performance metrics\n",
    "model = Sequential()\n",
    "model.add(Dense(best_overall_model.NeuronsLayer1, activation='relu', kernel_initializer='lecun_uniform',kernel_regularizer='l1',\n",
    "                input_shape=(n_features,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(best_overall_model.NeuronsLayer2, activation='relu', kernel_initializer='lecun_uniform',kernel_regularizer='l1'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics='binary_accuracy')\n",
    "\n",
    "model.fit(features_upsampled, labels_upsampled,\n",
    "              epochs=1000,\n",
    "              batch_size=500,\n",
    "              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance metrics for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix,confusion_matrix, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "#Predict the labels for testing set\n",
    "predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "#Calculate f1 score and display\n",
    "print(f1_score(labelTest,predictions,average='binary'))\n",
    "\n",
    "#Calculate confusion matrix\n",
    "confusionMatrix = confusion_matrix(labelTest,predictions)\n",
    "\n",
    "figure = plt.figure(figsize=(7, 6))\n",
    "\n",
    "sns.heatmap(confusionMatrix, annot=True,cmap='Blues',fmt='d')\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label',fontsize=20)\n",
    "plt.xlabel('Predicted label',fontsize=20)\n",
    "plt.title('Truth Table Any Catastrophy Classification',fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "#Calculate performance metrics\n",
    "report = classification_report(labelTest,predictions,output_dict=True)\n",
    "report = pd.DataFrame(report).transpose()\n",
    "\n",
    "#Calculate ROC based on prediction probabilities\n",
    "predictions = model.predict(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labelTest, predictions)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "display_curve = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "display_curve.plot()  \n",
    "plt.show() \n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
